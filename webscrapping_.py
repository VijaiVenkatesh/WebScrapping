# -*- coding: utf-8 -*-
"""Webscrapping .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mldhgOx1jTRRfcP5X3LTVv56KhiMCBjA
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
def connect(values):
        con=sqlite3.connect("eBay.db")
        
        con.execute("CREATE TABLE IF NOT EXISTS EBAYWATCH(NAME TEXT,SUBTITLE TEXT,PRICE TEXT,LOCATION TEXT)")
        for val in values:
            #print(val)
            try:
                inq=con.execute("INSERT INTO EBAYWATCH (NAME,SUBTITLE,PRICE,LOCATION) VALUES (?,?,?,?)")
                con.execute(inq,val)
                
            except:
                pass   
        cur=con.cursor()
        cur.execute("SELECT * FROM EBAYWATCH")
        table=cur.fetchall()
        for watch in table:
            print(watch)
        con.commit()      
        con.close()      
num=int (input("ENTER THE PAGE NUMBER"))
scap=[]
for i in range(1,num+1):
    url=("https://www.ebay.com/sch/i.html?_from=R40&_nkw=watches&_sacat=0&_pgn="+str(num))
    req=requests.get(url)
    con=req.content
    soup=BeautifulSoup(con,"html.parser")
    alh=soup.find_all("div",{"class":"s-item__wrapper clearfix"})
    for i in alh:
        watch_dict = {}
        try:
            watch_dict["watch_name"]=i.find("h3",{"class":"s-item__title"}).text
        except:
                AttributeError
                pass
        try:
            watch_dict["Subtitle"]=i.find("div",{"class":"s-item__subtitle"}).text
        except:
                AttributeError
                pass
        try:
            watch_dict["Price"]=i.find("span",{"class":"s-item__price"}).text
        except:
                AttributeError
                pass
        try:
            watch_dict["Location"]=i.find("span",{"class":"s-item__location s-item__itemLocation"}).text
        except:
                AttributeError
                pass
        scap.append(watch_dict)  
connect(tuple(scap))    
dataframe=pd.DataFrame(scap)
dataframe.to_csv("eBayProject.csv")